# Towards Comprehensive Benchmarking of Medical Visionâ€“Language Models (Medâ€‘VLMs)

**A Unified Research Framework for Efficient, Trustworthy, and Clinically Deployable Medical Visionâ€“Language Models**

Medical imaging workflows rely on the integration of **radiology images** and **freeâ€‘text reports**. While Large Visionâ€“Language Models (LVLMs) such as GPTâ€‘4V and LLaVAâ€‘Med demonstrate strong medical reasoning, they remain challenging to deploy in real clinical environments due to:
- heavy computational requirements,
- privacy and dataâ€‘governance barriers,
- limited interpretability,
- reliance on cloudâ€‘scale infrastructure.

This motivates a systematic study of **small and domainâ€‘specific models (<10B parameters)**â€”including MedCLIP, BioMedCLIP, CheXzero, MedFILIP, MedBridge, and radiologyâ€‘specific SLMsâ€”which offer:
- lower latency,
- reduced VRAM requirements,
- improved transparency,
- onâ€‘premise feasibility for hospitals.

This repository provides the **benchmarking foundation** for the manuscript:  
ðŸ“„ *"Towards Comprehensive Benchmarking of Medical Vision Language Models"* îˆ€fileciteîˆ‚turn0file0îˆ

It aims to serve as a **researchâ€‘first, reproducible benchmark suite** for evaluating efficiency, accuracy, trustworthiness, and clinical readiness of Medâ€‘VLMs.

---

# ðŸ”­ Highâ€‘Level Research Overview
This project investigates three pillars of Medâ€‘VLM performance:

### **1. Task Performance**
- Zeroâ€‘shot classification (CheXpert labels)
- Multimodal retrieval (imageâ€“report / reportâ€“image)
- Report summarization and impression generation

### **2. Efficiency & Deployability**
- latency and throughput
- VRAM / CPU footprint
- model size, FLOPs, quantization behavior
- stability across seeds

### **3. Trustworthiness & Reliability**
- factual correctness
- calibration error
- robustness to perturbations
- rareâ€‘finding performance

This unified framework will later extend beyond chest Xâ€‘rays to CT/MRI/ophthalmology datasets.

---

# ðŸ§­ Project Phases
The repository is organized around the evolution of the research pipeline.

---

# ## **Phase 1 â€” Establishing Baseline Benchmarks (IU Chest Xâ€‘ray)**
### **Goal:** Build a reproducible, CPUâ€‘friendly baseline pipeline using publicly available data.

Phase 1 focuses on the **Indiana University Chest Xâ€‘ray dataset**, chosen because it is:
- fully public and deâ€‘identified,
- small enough for rapid iteration,
- paired with highâ€‘quality radiology reports,
- ideal for early CPUâ€‘level prototyping.

### **Models evaluated in Phase 1:**
- **MedCLIP** â€” contrastive imageâ€“text alignment
- **BioMedCLIP** â€” vision encoder + PubMedBERT
- **CheXzero** â€” zeroâ€‘shot classification

### **Tasks implemented:**
- image embedding extraction
- text embedding extraction
- cosineâ€‘similarity retrieval
- topâ€‘K ranking
- zeroâ€‘shot pathology classification

### **Outputs from Phase 1:**
- IUâ€‘Xray preprocessing and splits
- EDA notebooks + PDF reports
- endâ€‘toâ€‘end Kaggle GPU workflows
- reproducible MedCLIP/BioMedCLIP benchmarks
- baseline metrics for all Phaseâ€‘2 comparisons

This phase forms the foundation for scaling up to clinical datasets.

---

# ## **Phase 2 â€” Scaling to CheXpert and MIMICâ€‘CXR (Ongoing)**
### **Goal:** Build a comprehensive, clinically meaningful benchmark that assesses accuracy, efficiency, and trust.

Phase 2 expands the pipeline to:
- **CheXpert** (labelâ€‘rich, highâ€‘quality dataset)
- **MIMICâ€‘CXR v2.1.0** (largest public CXR dataset)

### **New components introduced:**
#### **1. Advanced EDA (global)**
- label frequency & imbalance
- metadata and device analysis
- density/intensity distributions

#### **2. Larger model families**
- LLaVAâ€‘Med
- XrayGPT
- MedBridge
- MedFILIP
- SLM baselines (BioClinicalBERT, TinyBERT, DistilBERT)

#### **3. Efficiencyâ€‘focused experiments**
- 8â€‘bit / 4â€‘bit quantization
- qLoRA fineâ€‘tuning
- throughput + latency benchmarking
- VRAM footprint tracking

#### **4. Trustworthiness assessment**
- factual alignment
- calibration metrics
- robustness to perturbations
- rareâ€‘finding performance

### **Expected Phaseâ€‘2 Outputs:**
- unified metrics tables (AUC, ROUGEâ€‘L, Recall@K)
- crossâ€‘dataset evaluation
- quantization & PEFT ablation studies
- trustworthiness report
- comparison across model architectures

Phase 2 will form the basis for the main results in the final paper.

---

# ## **Initial POC Experiments (Local Prototyping)
Before formalizing Phase 1, small exploratory experiments were run locally to:
- validate loaders,
- test preprocessing variations (PIL vs OpenCV),
- run mini retrieval experiments,
- build a first prototype for the MedCLIP/BioMedCLIP pipeline,
- verify Kaggle GPU compatibility.

These experiments informed the more structured pipelines found in Phase 1.

---

# ðŸ“ Model Parameter Comparison (Current Baselines)
| Model | Architecture | Parameters | Core Capability |
|-------|-------------|------------|-----------------|
| **MedCLIP** | ViT-Base + BioClinicalBERT | â‰ˆ 86M | Imageâ€“text alignment |
| **BioMedCLIP** | ViT-Base + PubMedBERT | â‰ˆ 120M | Cross-modal retrieval |
| **CheXzero** | ResNet-50 + Domain LM | â‰ˆ 90M | Zero-shot classification |
| **LLaVA-Med / XrayGPT** | Vision encoder + LLM decoder | 7B+ | Report reasoning & summarization |

This comparison highlights the accuracyâ€“efficiency trade-offs motivating our focus on **small, deployable Med-VLMs**.

---

# ðŸ§© Modular Benchmarking Framework
MedVLM Bench is designed as a **modular, extensible research toolkit**.

Each baseline model has:
- **Config files** in `configs/`
- **Dedicated scripts** in `scripts/` or phase-specific `notebooks/`
- **Metrics & outputs** tracked in `reports_phase1/`, `EDA/`, or model-specific outputs

### Adding New Tasks
New tasks (e.g., projection learning, disease-label extensions, advanced retrieval, RadGraph entity extraction) can be added as plug-in modules following the structure of existing scripts such as:
- `medclip_demo.py`
- `projection_medclip.py`

This modular design supports Phase 2 expansion and future multi-dataset evaluation.

---

---

# ðŸ—‚ Repository Organization
```
MedVLMBench/
â”‚
â”œâ”€â”€ data/                      # Local datasets (ignored in Git)
â”‚
â”œâ”€â”€ EDA/                       # Global EDA notebooks + PDF reports
â”‚   â”œâ”€â”€ notebooks_eda/
â”‚   â””â”€â”€ eda_reports/
â”‚
â”œâ”€â”€ docs/                      # Workflow docs + project overview
â”‚
â”œâ”€â”€ phase1/                    # IU-Xray baseline pipeline
â”‚   â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ reports_phase1/
â”‚   â””â”€â”€ medvlm_core/
â”‚
â”œâ”€â”€ phase2/                    # CheXpert + MIMIC-CXR benchmark (in progress)
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements_kaggle.txt
â””â”€â”€ README.md
```

---

# âš™ï¸ Setup Instructions
### **1 â€” Clone the repository**
```bash
git clone https://github.com/dimplek0424/MedVLMBenchPhase1.git
cd MedVLMBenchPhase1
```

### **2 â€” Create conda environment**
```bash
conda create -n medvlm python=3.10 -y
conda activate medvlm
```

### **3 â€” Install dependencies**
```bash
pip install -r requirements.txt
```
For Kaggle:
```bash
pip install -r requirements_kaggle.txt
```

---

# ðŸ“Š Evaluation Tasks & Metrics
| Task | Description | Metrics |
|------|-------------|---------|
| Zeroâ€‘Shot Classification | Predict CheXpert labels | AUC, F1, Accuracy |
| Crossâ€‘Modal Retrieval | Image â†” Report search | Recall@K, Cosine Similarity |
| Report Summarization | Generate clinical impressions | ROUGEâ€‘L, BLEU, BERTScore |
| Efficiency Analysis | Measure deployability | VRAM, latency, FLOPs |

---

# ðŸ›¡ Ethics & Compliance
- Only uses public, deâ€‘identified datasets
- Complies with PhysioNet DUA
- No PHI or sensitive information stored
- All models follow their original licenses

---

# ðŸ“š References
Formal citations and expanded methodology appear in the draft manuscript:  
ðŸ“„ *"Towards Comprehensive Benchmarking of Medical Vision Language Models"* îˆ€fileciteîˆ‚turn0file0îˆ

---

# ðŸ‘©â€ðŸ’» Maintainer
**Dimple Khatri** â€” AI for Healthcare Researcher  
Contact: dimplek0424@gmail.com
