# ü©∫ Project Overview ‚Äî MedVLM Phase 1  
**Title:** Towards Comprehensive Benchmarking of Medical Vision‚ÄìLanguage Models (VLMs)  
**Principal Investigator:** Dimple Khatri  
**Mentor:** Dr. Sanjan T. P. Gupta (AI for Healthcare)  
**Current Phase:** Local benchmarking using Indiana University Chest X-Ray dataset  
**Planned Conference Submission:** ISCB Asia 2025  

---

## 1Ô∏è‚É£ Research Objectives
- Build reproducible baselines for **medical VLMs** (e.g., MedCLIP, BioMedCLIP, CheXzero).  
- Evaluate models for **zero-shot classification**, **multimodal retrieval**, and **report summarization**.  
- Study **accuracy vs. efficiency** trade-offs on CPU-only setups.  
- Design scalable workflows for eventual integration with **MIMIC-CXR** under data-use compliance.  

---

## 2Ô∏è‚É£ Project Phases

| Phase | Dataset | Purpose | Notes |
|:------|:---------|:--------|:------|
| **Phase 1** | [Indiana University Chest X-Ray (Open-i / Kaggle)](https://www.kaggle.com/datasets/raddar/chest-xrays-indiana-university) | Lightweight local benchmarking | Enables experimentation without institutional restrictions |
| **Phase 2** | [MIMIC-CXR v2.1.0 (PhysioNet)](https://physionet.org/content/mimic-cxr/2.1.0/) | Scalable benchmarking with real clinical data | Requires DUAs + ethics approval |
| **Phase 3** | Quantized + Explainable VLMs | Efficiency & interpretability | Integration with Grad-CAM / LoRA / attention maps |

---

## 3Ô∏è‚É£ Repository Structure
MedVLMPhase1/
‚îú‚îÄ‚îÄ data/ # Local dataset storage (excluded)
‚îú‚îÄ‚îÄ notebooks/
‚îÇ ‚îú‚îÄ‚îÄ medclip_demo.py ‚Üí CPU-safe MedCLIP baseline
‚îÇ ‚îú‚îÄ‚îÄ biomedclip_local.py ‚Üí BioMedCLIP evaluation
‚îÇ ‚îú‚îÄ‚îÄ evaluate_vlm_models.py ‚Üí Task-wise comparison script
‚îÇ
‚îú‚îÄ‚îÄ outputs/ # Model embeddings, metrics (excluded)
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ PROJECT_OVERVIEW.md # ‚Üê this file

---

## 4Ô∏è‚É£ Model Registry and Links

| Model | Paper / Repo | Domain | Key Capability |
|:------|:--------------|:-------|:---------------|
| **MedCLIP** | [GitHub](https://github.com/UCSD-AI4H/MedCLIP) ¬∑ [arXiv 2023](https://arxiv.org/abs/2301.01558) | Radiology | Image‚Äìtext alignment |
| **BioMedCLIP** | [Hugging Face](https://huggingface.co/microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224) | Biomedical | Cross-modal retrieval |
| **CheXzero** | [GitHub](https://github.com/rajpurkarlab/chexzero) ¬∑ [Nature Med 2022](https://www.nature.com/articles/s41591-022-01920-1) | Chest X-Ray | Zero-shot classification |
| **LLaVA-Med** | [GitHub](https://github.com/microsoft/LLaVA-Med) ¬∑ [arXiv 2024](https://arxiv.org/abs/2403.18798) | Biomedical | Vision-language reasoning |
| **XrayGPT** | [GitHub](https://github.com/UCSD-AI4H/XrayGPT) | Radiology | Report summarization + QA |

---

## 5Ô∏è‚É£ Evaluation Tasks and Metrics
| Task | Metrics | Description |
|:------|:---------|:-------------|
| **Zero-Shot Classification** | Accuracy, F1, AUC | Label prediction without fine-tuning |
| **Retrieval (Img ‚Üî Text)** | Recall@K, Cosine Sim | Cross-modal embedding alignment |
| **Report Summarization** | BLEU, ROUGE-L, BERTScore | Textual coherence & semantic fidelity |
| **Efficiency Analysis** | Inference Time, Memory Use | Resource-aware evaluation on CPU |

---

## 6Ô∏è‚É£ Ethical and Data Governance
- Only **publicly available datasets** are used in this phase.  
- No PHI or identifiable clinical data are stored or shared.  
- All models and data follow their respective **licenses and DUAs**.  

---

## 7Ô∏è‚É£ Next Steps
- Deploy MedCLIP/BioMedCLIP on full MIMIC-CXR dataset (Phase 2).  
- Introduce **LoRA fine-tuning** and **quantization**.  
- Extend benchmarks to **XrayGPT / LLaVA-Med** for report generation.  
- Submit full paper to ISCB Asia 2025 or alternate venue.  

---

## üßæ License
This repository is open-source for research and educational use under the **MIT License**.  
Please cite appropriately when using any component.
